
<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="SoMeLVLM">
  <meta name="keywords" content="LLM, Large Vision Language Model, Computational Social Science, Social Media">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SoMeLVLM: A Large Vision Language Model for Social Media Processing</title>

<!-- Begin Jekyll SEO tag v2.8.0 -->
<!-- <meta name="generator" content="Jekyll v3.9.3" /> -->
<meta property="og:title" content="SoMeLVLM: A Large Vision Language Model for Social Media Processing" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://somelvlm.github.io/" />
<meta property="og:url" content="https://somelvlm.github.io/" />
<meta property="og:site_name" content="SoMeLVLM: A Large Vision Language Model for Social Media Processing" />
<meta property="og:type" content="website" />
<meta property="og:image" content="https://llm-misinformation.github.io/static/images/logo.png" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="SoMeLVLM: A Large Vision Language Model for Social Media Processing" />
<meta name="twitter:description" content="SoMeLVLM: A Large Vision Language Model for Social Media Processing" />
<meta name="twitter:site" content="@CanyuChen3" />
<meta name="twitter:image" content="https://llm-misinformation.github.io/static/images/logo.png" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","headline":"LLMs Meet Misinformation","name":"LLMs Meet Misinformation","url":"https://llm-misinformation.github.io/"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    .grey-box {
        background-color: #c0c0c0; /* Grey color */
        color: rgb(70, 70, 70); /* White text color */
        padding: 20px; /* Padding inside the box */
        margin: 20px; /* Margin outside the box */
        text-align: center; /* Center the text */
    }
  </style>

</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="http://www.fudan-disc.com/">
        <span class="icon">
            <i class="fas fa-home"></i>
        </span>
        </a>
  
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Open Source Models
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="http://med.fudan-disc.com/">
              DISC-MedLLM
            </a>
            <a class="navbar-item" href="http://law.fudan-disc.com/">
              DISC-LawLLM
            </a>
            <a class="navbar-item" href="http://fin.fudan-disc.com/">
              DISC-FinLLM
            </a>
          </div>
        </div>
      </div>
  
    </div>
  </nav>

  <!-- <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img src="static/images/logo_1.png" alt="LLMs Meet Misinformation" width="900"/>
            <h1 class="title is-4 publication-title">This is an initiative aiming to combat misinformation in the age of LLMs
            </h1>
            <h1 class="title is-5 publication-title">(Contact: <a href="https://canyuchen.com/">Canyu Chen</a>)
            </h1>
            <ul>

              <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
              <div class="content has-text-justified">
              <a href="#Combating-Misinformation-in-the-Age-of-LLMs-Opportunities-and-Challenges">Combating Misinformation in the Age of LLMs: Opportunities and Challenges</a>
              <br>
               - A survey of the opportunities (<i>can we utilize LLMs to combat misinformation</i>) and challenges (<i>how to combat LLM-generated misinformation</i>) of combating misinformation in the age of LLMs.
              <br>
              <a href="#Can-LLM-Generated-Misinformation-Be-Detected"> (ICLR 2024) Can LLM-Generated Misinformation Be Detected?</a>
              <br>
              - We discover that LLM-generated misinformation can be <i>harder</i> to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm.
              <br>
              </div>
              </div>
              </div>

            </ul>
          </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  


  <!-- <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 id="Combating-Misinformation-in-the-Age-of-LLMs-Opportunities-and-Challenges" class="title is-2 publication-title">Combating Misinformation in the Age of LLMs: Opportunities and Challenges
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://canyuchen.com
              ">Canyu Chen</a>,</span>
              <span class="author-block">
                <a href="http://www.cs.iit.edu/~kshu/
              ">Kai Shu</a></span>
              <span class="author-block">
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Illinois Institute of Technology</span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2311.05656.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2311.05656" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/llm-misinformation/llm-misinformation-survey"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Paper List</span>
                    </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section> -->

  
  <!-- <section class="section">
    <div class="container is-max-desktop">
      <figure>
        <img src="static/images/survey.png" alt="survey">
      </figure>
    </div>
    <br>    <br>
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>Misinformation such as fake news and rumors is a serious threat to information ecosystems 
              and public trust. The emergence of Large Language Models (LLMs) has great potential to reshape 
              the landscape of combating misinformation. Generally, LLMs can be a double-edged sword in the fight. 
              On the one hand, LLMs bring promising opportunities for combating misinformation due to their profound 
              world knowledge and strong reasoning abilities. Thus, one emergent question is: <i><b>can we utilize LLMs to 
              combat misinformation?</b></i> On the other hand, the critical challenge is that LLMs can be easily leveraged 
              to generate deceptive misinformation at scale. Then, another important question is: <i><b>how to combat 
              LLM-generated misinformation?</b></i> In this paper, we first systematically review the history of combating 
              misinformation before the advent of LLMs. Then we illustrate the current efforts and present an outlook 
              for these two fundamental questions respectively. The goal of this survey paper is to facilitate the 
              progress of utilizing LLMs for fighting misinformation and call for interdisciplinary efforts from 
              different stakeholders for combating LLM-generated misinformation.
          </div>
        </div>
      </div>
    </div>
  </section> -->


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{chen2023combating,
      title   = {Combating Misinformation in the Age of LLMs: Opportunities and Challenges},
      author  = {Canyu Chen and Kai Shu},
      year    = {2023},
      journal = {arXiv preprint arXiv: 2311.05656}
    }</code></pre>
  </div>
</section> -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 id="SoMeLVLM: A Large Vision Language Model for Social Media Processing" class="title is-2 publication-title">SoMeLVLM: A Large Vision Language Model for Social Media Processing
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://lishi905.github.io/">Xinnong Zhang</a><sup>*1</sup>,</span>
              <span class="author-block">
                <a href="https://haoyuk.github.io/">Haoyu Kuang</a><sup>*1</sup>,</span>
              <span class="author-block">
                <a href="http://www.fudan-disc.com/people">Xinyi Mou</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://brucelyu17.github.io/">Hanjia Lyu</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="http://www.fudan-disc.com/people">Kun Wu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="http://simingchen.me/">Siming Chen</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://nlp.fudan.edu.cn/28702/list.htm">Xuanjing Huang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="http://www.fudan-disc.com/people/zywei">Zhongyu Wei</a><sup>1</sup>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Fudan University,</span>
              <span class="author-block"><sup>2</sup>University of Rochester</span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2402.13022.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2402.13022" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/Lishi905/SoMeLVLM"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/Lishi0905/SoMeLVLM"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="far fa-images"></i>
                    </span>
                    <span>Model</span>
                    </a>
                <!-- <span class="link-block">
                  <a href="https://zhuanlan.zhihu.com/p/678425256"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-zhihu"></i>
                    </span>
                    <span>post</span>
                    </a>
                </span> -->
                <!-- <span class="link-block">
                  <a href="https://x.com/CanyuChen3/status/1749337997340790955?s=20"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-twitter"></i>
                    </span>
                    <span>post</span>
                    </a>
                </span> -->
                <!-- <span class="link-block">
                  <a href="https://www.linkedin.com/posts/canyu-chen-1b2415100_iclr2024-misinformation-llm-activity-7155088736353972224--5Ng?utm_source=share&utm_medium=member_desktop"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-linkedin"></i>
                    </span>
                    <span>post</span>
                    </a>
                </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="columns is-centered"> -->
            <!-- <img style='height: auto; width: 50%; object-fit: contain' src="static/images/f1.png"
              alt="overview_image"> -->
      <figure>
        <img src="static/images/framework.png"  alt="survey" >
      </figure>
          <!-- </div>
        </div>
      </div>  -->
    </div>
    <br>    <br>
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              The growth of social media, characterized by its multimodal nature, has led to the emergence 
              of diverse phenomena and challenges, which calls for an effective approach to uniformly solve 
              automated tasks. The powerful Large Vision Language Models make it possible to handle a variety 
              of tasks simultaneously, but even with carefully designed prompting methods, the general domain 
              models often fall short in aligning with the unique speaking style and context of social media tasks. 
              </p>
              <p>
              In this paper, we introduce a Large Vision Language Model for Social Media Processing (SoMeLVLM), 
              which is a cognitive framework equipped with five key capabilities including 
              <i><b>knowledge & comprehension</b></i>, <i><b>application</b></i>, <i><b>analysis</b></i>, 
              <i><b>evaluation</b></i>, and <i><b>creation</b></i>. SoMeLVLM is designed to understand and generate 
              realistic social media behavior. We have developed a 654k multimodal social media instruction-tuning 
              dataset to support our cognitive framework and fine-tune our model. Our experiments demonstrate that 
              SoMeLVLM achieves state-of-the-art performance in multiple social media tasks. Further analysis shows 
              its significant advantages over baselines in terms of cognitive abilities.
              </p>
          </div>
        </div>
      </div>
    </div>
  <!-- </section>
  <section class="section"> -->
    <br>
    <br>
    <div class="container is-max-desktop">
      <!-- Method -->
      <!-- <br /> -->
      <div style="text-align:center">
        <h2 class="title is-3">Our Contributions</h2>
      </div>
      <div class="content has-text-justified">
        <br>
            <p>We propose a large vision language model specifically tailored for <i><b>social media</b></i> contexts, 
              capable of delivering high-quality text classification and interpretation under zero-shot conditions, 
              fundamentally simplifying the research workflow in computational social science and improving overall reliability.</p>
            <p>We construct a comprehensive social media framework by combining <b>cognitive abilities</b> with traditional 
              social media tasks to support different levels of demands in information processing.</p>
            <p>We contribute to a large-scale, high-quality multimodal social media dataset, encompassing both pure text 
              and multimodal formats, with data from both open-source and self-collected sources, formatted into diverse 
              instruction-tuning formats.</p>
        <br>
        <br>
  

      <div style="text-align:center">
        <h2 class="title is-3">Challenges Faced by General Domain Models</h2>
      </div>
      <div class="content has-text-justified">
        <br>
        <p><b>Limitations in social multimedia understanding</b>: General domain LLMs or LVLMs tend to focus more on text over other modalities, 
          which is <b>not</b> consistent with real-world user habits on social media. Social media tasks often require fine-grained recognition 
          ability to combine captions and images from a single post and synthesize the user's intention. Genereal domain large models may not 
          possess this level of nuanced multimodal understanding, as shown in Figure (a). </p>
        <p><b>Challenges in informal language understanding</b>: There is a huge gap between the informal speaking style prevalent on social media 
          and the formal language used in other contexts. As a result, general domain LLMs and LVLMs fall short in recognizing sentiment, humor, 
          figurative language, and other related concepts when the sentences are expressed casually. The example shown in Figure (b) 
          demonstrates that the model cannot recognize the wordplay <i><b>banded</b></i> in the user's post. </p>
        <p><b>Complex cognitive demands in social media tasks</b>: Social media tasks often involve multiple objectives to address high-level 
          social demands that require a combination of complex cognitive abilities and information-processing levels. For instance, the detoxifying 
          task illustrated in Figure (c), involves both hate speech detection and content rewriting. However, the models without 
          these abilities struggle to comprehensively address these aspects, resulting in less than satisfactory outputs.</p>
      </div>
      <div class="columns is-centered">
        <!-- <img style='height: auto; width: 90%; object-fit: contain' src="static/images/f2.png"
          alt="overview_image"> -->
          <figure>
            <img src="static/images/intro.png" style="width:80%" alt="Figure 1">
          </figure>
      </div>
      <br>


      <br />
      <div style="text-align:center">
        <h2 class="title is-3">Datasets</h2>
      </div>
      <div class="content has-text-justified">
        <br>
      <p>
        We have develop a 654k social media dataset SoMeData, which consists of five cognitive modules and various CSS task categories.
      </p>
      </div>
      <div class="columns is-centered">
        <!-- <img style='height: auto; width: 90%; object-fit: contain' src="static/images/table1.png"
          alt="overview_image"> -->
          <figure>
            <img src="static/images/table1_new.png" style="width:80%" alt="Figure 2">
          </figure>
      </div>
      <br> 

      <br />
      <div style="text-align:center">
        <h2 class="title is-3">Framework Design</h2>
      </div>
      <div class="content has-text-justified">
        <br>
        <p>To construct a large vision language model capable of understanding and creating multimodal content on social media, 
          we consider concepts from cognitive teaching methods and build a comprehensive multimodal social media cognitive framework.
          We begin by designing a cognitive pyramid according to <a href="https://web.archive.org/web/20201212072520id_/https://www.uky.edu/~rsand1/china2018/texts/Bloom%20et%20al%20-Taxonomy%20of%20Educational%20Objectives.pdf">Bloom's Taxonomy</a>, 
          which is a classic teaching theory proposed by Benjamin Bloom in 1956. The pyramid contains five cognitive levels: 
          <i><b>Knowledge & Comprehension</b></i>, <i><b>Application</b></i>, <i><b>Analysis</b></i>, <i><b>Evaluation</b></i>, 
          and <i><b>Creation</b></i>.</p>
        <p><b>Knowledge & Comprehension Level</b>: This level means to recall and understand basic facts. It represents a basic cognitive 
          ability in our framework. The instruction construction of this level consists of various <b>classification</b> tasks within the 
          context of social media. </p>
        <p><b>Application Level</b>: The Application level means to use the information in new situations, which is related to active 
          involvement in social media. The instruction construction is to make accurate <b>interpretations</b> based on the given ground 
          truth over various social media domains, implying an understanding of the reasons behind the labels. </p>
        <p><b>Analysis Level</b>: The analysis level requires the model to analyze the label and furnish the corresponding 
          interpretations independently. We aim for the model to offer explanations <b>in the absence of ground truth labels</b> 
          at this level.</p>
        <p><b>Evaluation Level</b>: At the evaluation level, we pay special attention to the existing prejudices within the data 
          and the abnormal behavior on social media. The construction of the data is divided into two aspects: (1) we undertake 
          detoxification or depolarization for abnormal texts. (2) we instruct the model to explain the underlying reasons for texts or 
          text-image pairs labeled as Misinformation. </p>
        <p><b>Creation Level</b>: The Creation level means to create reliable content related to social media. We tackle this demand 
          by setting <b>reverse</b> and <b>creation</b> tasks, respectively. In the <b>reverse</b> task, we require the model 
          to generate opposing viewpoints based on a specified topic and text. In the <b>create</b> task, the task is formulated 
          as the generation of new hashtags on social media.</p>
      <br>

      <br />
      <div style="text-align:center">
        <h2 class="title is-3">Experiment Setting</h2>
      </div>
      <div class="content has-text-justified">
        <br>
        <p><b>Data Split</b>:
          We fine-tune our model using around 564k training data and evaluate our SoMeLVLM across various aspects of social media, 
          including 14 multimodal datasets and 12 held-out plain text datasets, totaling around 89k data. 
          </p>
        <p><b>Evaluation Metrics</b>: For classification (CLS) tasks, we report the accuracy (Acc) of test results, which involves 
          string matching after proper processing. For generative (GEN) tasks, we report on automatic metrics such as BLEU and ROUGE. 
          In addition, we employ GPT-4 as a grading assistant through specific prompts to evaluate the test outcomes by scoring the 
          response on a scale from 0 to 5, where a higher score signifies greater consistency with the ground truth.
        </p>
        <p><b>Experiment Result Analysis</b>: We collect results according to the cognitive abilities mentioned in our framework. Specifically, 
          we collect the in-domain performance of multimodal parts (using overall Acc performance) and the OOD performance 
          of plain-text parts at the dataset level and categorize them into <i><b>Knowledge & Comprehension</b></i>, 
          <i><b>Application</b></i>, <i><b>Analysis</b></i>, <i><b>Evaluation</b></i>, and <i><b>Creation</b></i>, five cognitive levels in total.</p>
        <br>
      </div>
      <div class="columns is-centered">
        <!-- <img style='height: auto; width: 90%; object-fit: contain' src="static/images/f6.png"
          alt="overview_image"> -->
          <figure>
            <img src="static/images/radar.png" style="width:90%" alt="Figure 5">
          </figure>
      </div>

      <div class="content has-text-justified">
        <br>
        <p> </p>
        <p>Clearly, SoMeLVLM shows greater cognitive ability over baseline models in all of the cognitive levels. 
          At the multimodal <i><b>Creation</b></i> level, all of the models perform poorly as they are required to 
          generate three hashtags that best describe the post, which is not an easy task even for human beings.</p>
        <br>
      </div>
      <br>

      <br />
      <div style="text-align:center">
        <h2 class="title is-3">Conclusion</h2>
      </div>
      <div class="content has-text-justified">
        <br>
          <p>In our work, we introduce <i>SoMeLVLM</i>, a multimodal language model for social media processing, wherein we design five cognitive capabilities, 
          each of which is mapped to various levels of social media tasks.</p>
          <p>Building on this, we collect related plain text and multimodal datasets and enhance the capabilities of vision-language models on relevant 
          tasks through instruction tuning. Additionally, we construct an evaluation based on cognitive levels and test our model under zero-shot 
          conditions, comparing it with other advanced LLMs and LVLMs. The experimental results thoroughly demonstrate the superiority of our model. 
          Our work contributes to the computational social science field by providing methods for modeling and evaluating various tasks on social media 
          and a large-scale, high-quality multimodal social media dataset.</p>
          <!-- <p><b>Implication 1</b>: our findings directly suggest that humans can be more susceptible to LLM-generated misinformation and detectors can be less effective in detecting LLM-generated misinformation compared with human-written misinformation. In other words, <b>LLM-generated misinformation can be more deceptive and potentially cause more harm</b>.</p>
          <p><b>Implication 2</b>: on the one hand, a large amount of hallucinated information is potentially generated by normal users due to the popularity of LLMs. On the other hand, malicious users are more likely to exploit LLMs to generate misinformation to escape the detection of detectors. Thus, <b>there is a potential major paradigm shift of misinformation production from humans to LLMs</b>.</p>
          <p><b>Implication 3</b>: considering malicious users can easily prompt LLMs to generate misinformation at scale, which is more deceptive than human-written misinformation, online safety and public trust are faced with serious threats. <b>We call for collective efforts on combating LLM-generated misinformation from stakeholders</b> in different backgrounds including researchers, government, platforms, and the general public.</p> -->
      <br>        
      <br>


      <br />
      <div style="text-align:center">
        <h2 class="title is-3">Ethics Statement</h2>
      </div>
      <div class="content has-text-justified">
        <br>
          <p>The data used in this paper are from real users in diverse social media platforms, so the privacy problem
             is treated cautiously. The data from opensource datasets are safe as the sensitive information has already
              been masked. For the data we collect, we strictly follow the privacy policy of social media platforms 
              and will carefully avoid personal information before we release our instruction dataset.</p>
        <br>
      <br>
  </section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhang2024somelvlm,
      author = {Xinnong Zhang and Haoyu Kuang and Xinyi Mou and Hanjia Lyu and Kun Wu and Siming Chen and Jiebo Luo and Xuanjing Huang and Zhongyu Wei},
      title = {SoMeLVLM: A Large Vision Language Model for Social Media Processing},
      year = {2024},
      journal = {arXiv preprint arXiv: 2402.13022}
    }</code></pre>
  </div>
</section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

<script>
  function changeContent() {
    const dropdown = document.getElementById("dropdown");
    const selected = dropdown.value;
    const sections = ["example_1", "example_2", "example_3", "example_4", "example_5", "example_6", "example_7", "example_8", "example_9", "example_10", "example_11", "example_12", "example_13", "example_14"];

    sections.forEach((section) => {
      document.getElementById(section).style.display = (section === selected) ? "block" : "none";
    });
  }
</script>

</html>