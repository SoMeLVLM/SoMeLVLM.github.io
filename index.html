
<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="SoMeLVLM">
  <meta name="keywords" content="LLM, Large Vision Language Model, Computational Social Science, Social Media">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SoMeLVLM: A Large Vision Language Model for Social Media Processing</title>

<!-- Begin Jekyll SEO tag v2.8.0 -->
<!-- <meta name="generator" content="Jekyll v3.9.3" /> -->
<meta property="og:title" content="SoMeLVLM: A Large Vision Language Model for Social Media Processing" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://somelvlm.github.io/" />
<meta property="og:url" content="https://somelvlm.github.io/" />
<meta property="og:site_name" content="SoMeLVLM: A Large Vision Language Model for Social Media Processing" />
<meta property="og:type" content="website" />
<meta property="og:image" content="https://llm-misinformation.github.io/static/images/logo.png" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="SoMeLVLM: A Large Vision Language Model for Social Media Processing" />
<meta name="twitter:description" content="SoMeLVLM: A Large Vision Language Model for Social Media Processing" />
<meta name="twitter:site" content="@CanyuChen3" />
<meta name="twitter:image" content="https://llm-misinformation.github.io/static/images/logo.png" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","headline":"SoMeLVLM","name":"SoMeLVLM","url":"https://someLVLM.github.io/"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    .grey-box {
        background-color: #c0c0c0; /* Grey color */
        color: rgb(70, 70, 70); /* White text color */
        padding: 20px; /* Padding inside the box */
        margin: 20px; /* Margin outside the box */
        text-align: center; /* Center the text */
    }
  </style>

</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="">
        <span class="icon">
            <i class="fas fa-home"></i>
        </span>
        </a>
  
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Rearch
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://alarm-fdu.github.io/">
              ALaRM
            </a>
          </div>
        </div>
      </div>
  
    </div>
  </nav>
  

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 id="SoMeLVLM: A Large Vision Language Model for Social Media Processing" class="title is-2 publication-title">SoMeLVLM: A Large Vision Language Model for Social Media Processing
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://lishi905.github.io/">Xinnong Zhang</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="https://haoyuk.github.io/">Haoyu Kuang</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="https://xymou.github.io/">Xinyi Mou</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://brucelyu17.github.io/">Hanjia Lyu</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="http://www.fudan-disc.com/people">Kun Wu</a><sup>1</sup>,
              </span>
              <br/>
              <span class="author-block">
                <a href="http://simingchen.me/">Siming Chen</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://nlp.fudan.edu.cn/28702/list.htm">Xuanjing Huang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="http://www.fudan-disc.com/people/zywei">Zhongyu Wei</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Fudan University,</span>
              <span class="author-block"><sup>2</sup>University of Rochester</span>
            </div>
            <div class="is-size-6 publication-authors">
              <span class="author-block">* Equal contribution</span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2402.13022.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2402.13022" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/Lishi905/SoMeLVLM"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/Lishi0905/SoMeLVLM"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-folder"></i>
                    </span>
                    <span>Model</span>
                    </a>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="columns is-centered"> -->
            <!-- <img style='height: auto; width: 50%; object-fit: contain' src="static/images/f1.png"
              alt="overview_image"> -->
      <figure>
        <img src="static/images/framework.png"  alt="survey" >
      </figure>
      <div style="text-align:center">
      <div class="content has-text-justified">
        <p><i><b>An illustration of Our Social Media Cognitive Framework.</b> We build a cognitive pyramid based 
          on Bloom's Taxonomy, including cognitive levels of Knowledge & Comprehension, Application, Analysis,
          Evaluation, and Creation. These cognitive abilities are derived from different types of users on 
          social media and represent different levels of demands for information processing. </i></p>
      </div>
      </div>
          <!-- </div>
        </div>
      </div>  -->
    </div>
    <br>    <br>
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Background</h2>
          <div class="content has-text-justified">
            <p>
              The growth of social media, characterized by its multimodal nature, has led to the emergence 
              of diverse phenomena and challenges, which calls for an effective approach to uniformly solve 
              automated tasks. 
              </p>
              <p>
              The powerful Large Vision Language Models make it possible to handle a variety 
              of tasks simultaneously, but even with carefully designed prompting methods, the general domain 
              models often fall short in aligning with the unique speaking style and context of social media tasks. 
              </p>
              <p>
              In this paper, we introduce a Large Vision Language Model for Social Media Processing (SoMeLVLM), 
              which is a cognitive framework equipped with five key capabilities including 
              <i><b>knowledge & comprehension</b></i>, <i><b>application</b></i>, <i><b>analysis</b></i>, 
              <i><b>evaluation</b></i>, and <i><b>creation</b></i>. 
              <!-- SoMeLVLM is designed to understand and generate realistic social media behavior.  -->
              <!-- We have developed a 654k multimodal social media instruction-tuning 
              dataset to support our cognitive framework and fine-tune our model. Our experiments demonstrate that 
              SoMeLVLM achieves state-of-the-art performance in multiple social media tasks. Further analysis shows 
              its significant advantages over baselines in terms of cognitive abilities. -->
              </p>
          </div>
        </div>
      </div>
    </div>
  <!-- </section>
  <section class="section"> -->
    <br>
    <br>
    <div class="container is-max-desktop">
      <!-- Method -->
      <!-- <br /> -->
      <div style="text-align:center">
        <h2 class="title is-3">Our Contributions</h2>
      </div>
      <div class="content has-text-justified">
        <br>
            <p>
              We discover three major challenges faced by general domain models in addressing the nuances of social media: <b>Limitations in social 
                multimedia understanding</b>(Figure (a)), <b>Challenges in informal language understanding</b>(Figure (b)), <b>Unable to Cope with 
                Complex cognitive demands in social media tasks</b>(Figure (c)). Overall, the contributions of our paper are as follows: 
            </p>
            <ul>
              <li><b>Large Vision Language Model SoMeLVLM</b>: We propose a large vision language model specifically tailored for <i><b>social media</b></i> contexts, 
                capable of delivering high-quality text classification and interpretation under zero-shot conditions, 
                fundamentally simplifying the research workflow in computational social science and improving overall reliability.</li>
              <li><b>Social Media Cognitive Pyramid</b>: We construct a comprehensive social media framework by combining <i><b>cognitive abilities</b></i> with traditional 
                social media tasks to support different levels of demands in information processing.</li>
              <li><b>High-quality Multimodal Dataset</b>: We contribute to a large-scale, high-quality <i><b>multimodal social media 
                dataset</b></i>, encompassing both pure text and multimodal formats, with data from both open-source and self-collected 
                sources, formatted into diverse instruction-tuning formats.</li>
           </ul>
            <!-- <p>- We propose a large vision language model specifically tailored for <i><b>social media</b></i> contexts, 
              capable of delivering high-quality text classification and interpretation under zero-shot conditions, 
              fundamentally simplifying the research workflow in computational social science and improving overall reliability.</p>
            <p>- We construct a comprehensive social media framework by combining <i><b>cognitive abilities</b></i> with traditional 
              social media tasks to support different levels of demands in information processing.</p>
            <p>- We contribute to a large-scale, high-quality <i><b>multimodal social media dataset</b></i>, encompassing both pure text 
              and multimodal formats, with data from both open-source and self-collected sources, formatted into diverse 
              instruction-tuning formats.</p> -->
        <br>
        <br>
  

      <!-- <div style="text-align:center">
        <h2 class="title is-3">Challenges Faced by General Domain Models</h2>
      </div>
      <div class="content has-text-justified">
        <br>
        <p><b>Limitations in social multimedia understanding</b>: General domain LLMs or LVLMs tend to focus more on text over other modalities, 
          which is <b>not</b> consistent with real-world user habits on social media. Genereal domain large models may not possess this level 
          of nuanced multimodal understanding, as shown in Figure (a). </p>
        <p><b>Challenges in informal language understanding</b>: There is a huge gap between the informal speaking style prevalent on social media 
          and the formal language used in other contexts. The example shown in Figure (b) demonstrates that the model cannot recognize the 
          wordplay <i><b>banded</b></i> in the user's post. </p>
        <p><b>Complex cognitive demands in social media tasks</b>: Social media tasks often involve multiple objectives to address high-level social demands.
          For instance, the detoxifying task illustrated in Figure (c), involves both hate speech detection and content rewriting. 
          However, the models without complex cognitive abilities struggle to comprehensively address these aspects.</p> 
      </div> -->

      <div class="columns is-centered">
        <!-- <img style='height: auto; width: 90%; object-fit: contain' src="static/images/f2.png"
          alt="overview_image"> -->
          <figure>
            <img src="static/images/intro.png" style="width:80%" alt="Figure 1">
          </figure>
      </div>
      <br>
      
      <br />
      <div style="text-align:center">
        <h2 class="title is-3">Framework Design</h2>
      </div>
      <div class="content has-text-justified">
        <br>
        <p>
          We design a cognitive pyramid according to <a href="https://web.archive.org/web/20201212072520id_/https://www.uky.edu/~rsand1/china2018/texts/Bloom%20et%20al%20-Taxonomy%20of%20Educational%20Objectives.pdf">Bloom's Taxonomy</a>, 
          which is a classic teaching theory proposed by Benjamin Bloom in 1956. The pyramid contains five cognitive levels: 
          <i><b>Knowledge & Comprehension</b></i>, <i><b>Application</b></i>, <i><b>Analysis</b></i>, <i><b>Evaluation</b></i>, 
          and <i><b>Creation</b></i>.</p>
        <ul>
          <li><b>Knowledge & Comprehension Level</b>: This level means to recall and understand basic facts. The instruction construction 
            of this level consists of various <b>classification</b> tasks within the context of social media.</li>
          <li><b>Application Level</b>: The Application level means to use the information in new situations. The instruction construction 
            is to make accurate <b>interpretations</b> based on the given ground truth over various social media domains, implying an 
            understanding of the reasons behind the labels.</li>
          <li><b>Analysis Level</b>: The analysis level requires the model to analyze the label and furnish the corresponding 
            interpretations independently. We aim for the model to offer explanations <b>in the absence of ground truth labels</b> 
            at this level.</li>
          <li><b>Evaluation Level</b>: At the evaluation level, we pay special attention to the existing prejudices within the data 
            and the abnormal behavior on social media. The construction of the data is divided into two aspects: (1) we undertake 
            detoxification or depolarization for abnormal texts. (2) we instruct the model to explain the underlying reasons for texts or 
            text-image pairs labeled as Misinformation.
          </li>
          <li>
            <b>Creation Level</b>: The Creation level means to create reliable content related to social media. We tackle this demand 
            by setting <b>reverse</b> and <b>creation</b> tasks. In the <b>reverse</b> task, we require the model 
            to generate opposing viewpoints based on a specified topic and text. In the <b>create</b> task, the task is formulated 
            as the generation of new hashtags on social media.
          </li>
        </ul>
          <!-- <p><b>Knowledge & Comprehension Level</b>: This level means to recall and understand basic facts. The instruction construction 
            of this level consists of various <b>classification</b> tasks within the context of social media. </p>
          <p><b>Application Level</b>: The Application level means to use the information in new situations. The instruction construction 
            is to make accurate <b>interpretations</b> based on the given ground truth over various social media domains, implying an 
            understanding of the reasons behind the labels. </p>
          <p><b>Analysis Level</b>: The analysis level requires the model to analyze the label and furnish the corresponding 
            interpretations independently. We aim for the model to offer explanations <b>in the absence of ground truth labels</b> 
            at this level.</p>
          <p><b>Evaluation Level</b>: At the evaluation level, we pay special attention to the existing prejudices within the data 
            and the abnormal behavior on social media. The construction of the data is divided into two aspects: (1) we undertake 
            detoxification or depolarization for abnormal texts. (2) we instruct the model to explain the underlying reasons for texts or 
            text-image pairs labeled as Misinformation. </p>
          <p><b>Creation Level</b>: The Creation level means to create reliable content related to social media. We tackle this demand 
            by setting <b>reverse</b> and <b>creation</b> tasks. In the <b>reverse</b> task, we require the model 
            to generate opposing viewpoints based on a specified topic and text. In the <b>create</b> task, the task is formulated 
            as the generation of new hashtags on social media.</p> -->
      <br>

      <br />
      <div style="text-align:center">
        <h2 class="title is-3">Datasets</h2>
      </div>
      <div class="content has-text-justified">
        <br>
      <p>
        We have develop a 654k social media dataset <b>SoMeData</b>, which consists of five cognitive modules and various CSS task categories.
      </p>
      </div>
      <div class="columns is-centered">
        <!-- <img style='height: auto; width: 90%; object-fit: contain' src="static/images/table1.png"
          alt="overview_image"> -->
          <figure>
            <img src="static/images/table1_new.png" style="width:80%" alt="Figure 2">
          </figure>
      </div>
      <br> 

      <br />
      <div style="text-align:center">
        <h2 class="title is-3">Experiment Results</h2>
      </div>

      <div class="content has-text-justified">
        <br>
        <!-- <p><b>Data Split</b>:
          We fine-tune our model using around 564k training data and evaluate our SoMeLVLM across various aspects of social media, 
          including 14 multimodal datasets and 12 held-out plain text datasets, totaling around 89k data. 
          </p>
        <p><b>Evaluation Metrics</b>: For classification (CLS) tasks, we report the accuracy (Acc) of test results, which involves 
          string matching after proper processing. For generative (GEN) tasks, we report on automatic metrics such as BLEU and ROUGE. 
          In addition, we employ GPT-4 to evaluate the test outcomes by scoring the response on a scale from 0 to 5, where a higher 
          score signifies greater consistency with the ground truth.
        </p> -->
        <!-- <b>Experiment Result Analysis</b>:  -->
        <p>We conduct both classification task and generation task on both plain text domain and multimodal domain. 
          Specifically, for tasks containing images, we choose <a href="https://huggingface.co/Salesforce/blip2-flan-t5-xl">Blip2</a>, 
          <a href="https://huggingface.co/Salesforce/instructblip-vicuna-7b">InstructBlip (both Vicuna-based and FlanT5xl-based)</a>, 
          <a href="https://huggingface.co/llava-hf/llava-1.5-7b-hf">Llava</a>, and <a href="https://huggingface.co/Vision-CAIR/MiniGPT-4">Minigpt4</a> 
          as our baseline models. And for tasks involving plain text, we select <a href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf">Llama-2-7b-chat-hf</a>, 
          <a href="https://huggingface.co/lmsys/vicuna-7b-v1.1">Vicuna-7b-v1.1</a>, and <a href="https://huggingface.co/THUDM/chatglm2-6b">ChatGLM2-6B</a> 
          as our baseline models.
        </p>
        <!-- <br> -->
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="control has-icons-left">
            <div class="select is-medium is-info is-rounded ">
              <select id="dropdown" onchange="changeContent()" style="width:auto">
                <option value="example_1">Results of Multimodal datasets</option>
                <option value="example_2">Results of Plain Text datasets</option>
              </select>
              <div class="icon is-small is-left">
                <!-- <i class="fas fa-comment-alt"></i> -->
                <i class="fas fa-table"></i>
              </div>
            </div>
         </div>
          <!--/ Dropdown -->

          <!-- Content -->
          <div id="example_1" class="content-section">
            <br />
            <div class="columns is-centered">
              <!-- <img style='height: auto; width: 90%; object-fit: contain' src="static/images/table1.png"
                alt="overview_image"> -->
                <figure>
                  <img src="static/images/m_cls.png" style="width:80%" alt="Figure 2">
                </figure>
            </div>
            
            <div class="columns is-centered">
              <!-- <img style='height: auto; width: 90%; object-fit: contain' src="static/images/table1.png"
                alt="overview_image"> -->
                <figure>
                  <img src="static/images/m_gen.png" style="width:80%" alt="Figure 2">
                </figure>
            </div>
          </div>

          <div id="example_2" class="content-section"  style="display:none">
            <br />
            <div class="columns is-centered">
              <!-- <img style='height: auto; width: 90%; object-fit: contain' src="static/images/table1.png"
                alt="overview_image"> -->
                <figure>
                  <img src="static/images/t_cls.png" style="width:80%" alt="Figure 2">
                </figure>
            </div>
            
            <div class="columns is-centered">
              <!-- <img style='height: auto; width: 90%; object-fit: contain' src="static/images/table1.png"
                alt="overview_image"> -->
                <figure>
                  <img src="static/images/t_gen.png" style="width:80%" alt="Figure 2">
                </figure>
            </div>
          </div>
        </div>
      </div>
    </div>

      <div class="content has-text-justified">
        <br>
        <p> </p>
      <p><b>Cognitive Abilities Analysis</b>: We collect results according to the cognitive abilities mentioned in our framework. Specifically, 
        we collect the in-domain performance of multimodal parts (using overall Acc performance) and the OOD performance 
        of plain-text parts at the dataset level and categorize them into <i><b>Knowledge & Comprehension</b></i>, 
        <i><b>Application</b></i>, <i><b>Analysis</b></i>, <i><b>Evaluation</b></i>, and <i><b>Creation</b></i>, five cognitive levels in total.</p>
      </div>
      <br>

      <div class="columns is-centered">
        <!-- <img style='height: auto; width: 90%; object-fit: contain' src="static/images/f6.png"
          alt="overview_image"> -->
          <figure>
            <img src="static/images/radar.png" style="width:90%" alt="Figure 5">
          </figure>
      </div>

      <div class="content has-text-justified">
        <br>
        <p> </p>
        <p>Clearly, SoMeLVLM shows greater cognitive ability over baseline models in all of the cognitive levels. 
          At the multimodal <i><b>Creation</b></i> level, all of the models perform poorly as they are required to 
          generate three hashtags that best describe the post, which is not an easy task even for human beings.</p>
        <br>
      </div>
      <br>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div style="text-align:center">
            <h2 class="title is-3">Demo Examples</h2>
          </div>
          <br />
          <div class="control has-icons-left">
            <div class="select is-medium is-info is-rounded ", width="80%">
              <select id="dropdown_2" onchange="changeVideo()" style="width:auto">
                <option value="video_1">Example 1: Knowledge & Comprehension</option>
                <option value="video_2">Example 2: Analysis</option>
                <option value="video_3">Example 3: Creation</option>
              </select>
              <div class="icon is-small is-left">
                <i class="fab fa-youtube"></i>
              </div>
            </div>
         </div>
          <!--/ Dropdown -->
          <!-- Content -->
          <div id="video_1" class="content-section">
            <br />
            <!-- <p>Analyze the emotions conveyed by images and text</p> -->
            <div class="columns is-centered">
            <video id="example_4" autoplay muted loop playsinline width="80%">
              <source src="./static/videos/demo2.mp4" type="video/mp4">
            </video>
          </div>
          </div>

          <div id="video_2" class="content-section"  style="display:none">
            <br />
            <div class="columns is-centered">
            <video id="example_5" autoplay muted loop playsinline width="80%">
              <source src="./static/videos/demo3.mp4" type="video/mp4">
            </video>
          </div>
          </div>

          <div id="video_3" class="content-section"  style="display:none">
            <br />
            <div class="columns is-centered">
            <video id="example_6" autoplay muted loop playsinline width="80%">
              <source src="./static/videos/demo1.mp4" type="video/mp4">
            </video>
          </div>
          </div>
        </div>
      </div>
    </div>

      <br />
      <div style="text-align:center">
        <h2 class="title is-3">Conclusion</h2>
      </div>
      <div class="content has-text-justified">
        <br>
          <p>In our work, we introduce <i><b>SoMeLVLM</b></i>, a multimodal language model for social media processing, wherein we design five cognitive capabilities, 
          each of which is mapped to various levels of social media tasks.</p>
          <p>Building on this, we collect related plain text and multimodal datasets and enhance the capabilities of vision-language models on relevant 
          tasks through instruction tuning. Additionally, we construct an evaluation based on cognitive levels and test our model under zero-shot 
          conditions, comparing it with other advanced LLMs and LVLMs. The experimental results thoroughly demonstrate the superiority of our model. 
          Our work contributes to the computational social science field by providing methods for modeling and evaluating various tasks on social media 
          and a large-scale, high-quality multimodal social media dataset.</p>
      <br>        
      <br>
      <!-- <br />
      <div style="text-align:center">
        <h2 class="title is-3">Ethics Statement</h2>
      </div>
      <div class="content has-text-justified">
        <br>
          <p>The data used in this paper are from real users in diverse social media platforms, so the privacy problem
             is treated cautiously. The data from opensource datasets are safe as the sensitive information has already
              been masked. For the data we collect, we strictly follow the privacy policy of social media platforms 
              and will carefully avoid personal information before we release our instruction dataset.</p>
        <br>
      <br> -->
  </section>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-3 publication-title">Related Works about Social Media</h1>
          <!-- <h1 class="title is-5 publication-title">(Contact: <a href="https://canyuchen.com/">Canyu Chen</a>) </h1> -->
          <ul>
            <div class="columns is-centered has-text-centered">
            <!-- <div class="column is-four-fifths"> -->
            <div class="column is-three-fourths">
            <div class="box">
            <div class="content has-text-justified">
            <a href="https://arxiv.org/abs/2311.07547">GPT-4V(ision) as A Social Media Analysis Engine</a>
            <br>
            - Hanjia Lyu, Jinfa Huang, Daoan Zhang, Yongsheng Yu, Xinyi Mou, Jinsheng Pan, Zhengyuan Yang, Zhongyu Wei, Jiebo Luo.
            <br>
            <br>
            <a href="https://arxiv.org/abs/2402.16333">Unveiling the Truth and Facilitating Change: Towards Agent-based Large-scale Social Movement Simulation</a>
            <br>
            - Xinyi Mou, Zhongyu Wei, Xuanjing Huang.
            <br>
            <br>
            <a href="https://lrec-coling-2024.org/accepted-papers/">PASUM: A Pre-training Architecture for Social Media User Modeling based on Text Graph</a>
            <br>
            - Kun Wu*, Xinyi Mou*, Lanqing Xue, Zhenzhe Ying, Weiqiang Wang, Qi Zhang, Xuanjing Huang, Zhongyu Wei.
            <br>
            <br>
            <a href="https://www2024.thewebconf.org/accepted/research-tracks/">Unifying Local and Global Knowledge: Empowering Large Language Models as Political Experts with Knowledge Graphs</a>
            <br>
            - Xinyi Mou, Zejun Li, Hanjia Lyu, Jiebo Luo, Zhongyu Wei.
            <br>
            <br>
            <a href="https://aclanthology.org/2021.acl-long.99/">Align Voting Behavior with Public Statements for Legislator Representation Learning</a>
            <br>
            - Xinyi Mou, Zhongyu Wei, Lei Chen, Shangyi Ning, Yancheng He, Changjian Jiang, Xuanjing Huang.
            </div>
            </div>
            </div>
            </div>
          </ul>
        </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhang2024somelvlm,
      author = {Xinnong Zhang and Haoyu Kuang and Xinyi Mou and Hanjia Lyu and Kun Wu and Siming Chen and Jiebo Luo and Xuanjing Huang and Zhongyu Wei},
      title = {SoMeLVLM: A Large Vision Language Model for Social Media Processing},
      year = {2024},
      journal = {arXiv preprint arXiv: 2402.13022}
    }</code></pre>
  </div>
</section>



  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

<script>
  function changeContent() {
    const dropdown = document.getElementById("dropdown");
    const selected = dropdown.value;
    const sections = ["example_1", "example_2"];

    sections.forEach((section) => {
      document.getElementById(section).style.display = (section === selected) ? "block" : "none";
    });
  }
</script>

<script>
  function changeVideo() {
    const dropdown = document.getElementById("dropdown_2");
    const selected = dropdown.value;
    const sections = ["video_1", "video_2", "video_3"];
    sections.forEach((section) => {
      document.getElementById(section).style.display = (section === selected) ? "block" : "none";
    });
  }
</script>

</html>
